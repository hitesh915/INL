% Metalearning with SVM in cvx% Keith Siilats and Margret Kjartansdottir Copyright, All rights reserved% Dataset - Three X variables (d=3).% X variables are normally distributed mean 0, stdev 1% Two Y variables.% Y variables are binary -1 and 1% Y1 and Y2 are all only generated from X1 and X2 so I hope SVM gives me% [1 1 0] as wn = 1000; %we split this into training, teacher and testingd=3;s = RandStream('mcg16807', 'Seed',0)RandStream.setDefaultStream(s)X = randn(n,d);randnoise = randn(n,1);Y1 = ((X(:,1)+X(:,2)+randnoise)>0)*2-1;%Y2 is different from Y1 but uses the same variablesrandnoise2 = randn(n,1);Y2 = ((5*X(:,1)-3*X(:,2)+randnoise2)>0)*2-1;Y=[Y1 Y2];% This is the cost of slack for SVM.C = 2;  %approx same magnitude as norm(w,2) when w is [1 1 0]%cvx converts to dual automatically, automatically installs sedumi solver%download http://cvxr.com/cvx/ then run 'cvx_setup' from matlab%I split data into two datasets, first 30 points and then points 31-150, In=30;Yt=Y(1:n,1); %pick the X and Y for the particular SVM inside the loopXt=X(1:n,:); %X is the same and Y is differentCt= norm(mean(abs(Xt))); %better estimate for Ccvx_begin %classical svm    variables wtrain(d) e(n) btrain    dual variable alphatrain    minimize( 0.5*wtrain'*wtrain + Ct*sum(e)) %norm(w) almost works except it takes an extra sqrt    subject to        Yt.*(Xt*wtrain+btrain)-1 +e >=0   :alphatrain;        e>=0; %slackcvx_end%same code in dualK=Xt*Xt'; %nxncvx_begin %dual    variables alphatrain2(n) %you don't have anything with size d    maximize( sum(alphatrain2) -  0.5*quad_form(Yt.*alphatrain2,K))    subject to       alphatrain2>=0       alphatrain2<=Ct       sum(alphatrain2.*Yt)==0cvx_endwtrain2=Xt'*(alphatrain2.*Yt);epsilon=0.0001;svii = find( alphatrain2 > epsilon & alphatrain2 < (Ct - epsilon));btrain2 =  (1/length(svii))*sum(Yt(svii) - K(svii,:)*alphatrain2.*Yt(svii));%testYtest=Y(ceil(end/2):end,1);Xtest=X(ceil(end/2):end,:);predictedY=sign(Xtest*wtrain+btrain);Ktest=Xtest*Xt';predictedY2=sign(Ktest*(alphatrain2.*Yt)+btrain2);'error rate'errororig=sum(Ytest~=predictedY)/size(Ytest,1)%verify'verify dual and primal solutions are the same'[sum(abs(wtrain-wtrain2)) sum(abs(alphatrain-alphatrain2)) sum(abs(predictedY-predictedY2))]%Steve Gunn's svm package%http://www.isis.ecs.soton.ac.uk/resources/svminfo/%In svc.m replace [alpha lambda how] = qp(?); with%[alpha lambda how] = quadprog(H,c,[],[],A,b,vlb,vub,x0);%then runker = 'linear';global p1 p2;p1=sigma;[nsv alpha3 bias]=svc(Xt,Yt,ker,Ct);predictedYgunn=svcoutput(Xt,Yt,Xtest,ker,alpha3,b);%introduce teachern=120;Yt=Y(31:31+n,2); %pick the X and Y for the particular SVM inside the loopXt=X(31:31+n,:); %X is the same and Y is differentCt= norm(mean(abs(Xt))); %better estimate for Ccvx_begin %classical svm    variables wteacher(d) e(n) bteacher    dual variable alphatrain    minimize( 0.5*wteacher'*wteacher + Ct*sum(e)) %norm(w) almost works except it takes an extra sqrt    subject to        Yt.*(Xt*wteacher+bteacher)-1 +e >0   :alphateacher;        e>=0; %slackcvx_end%rerun original model with the teachern=30;Yt=Y(1:n,1); %pick the X and Y for the particular SVM inside the loopXt=X(1:n,:); %X is the same and Y is differentCt= norm(mean(abs(Xt))); %better estimate for Ccvx_begin    variables w(d) e(n) b    dual variable alphataught    %metalearning rate - we use w of Y2 to learn w of Y1    gamma=5*Ct; %probably too high for practice but illustrates the point    %switch off(or penalize) features the teacher didnt like    s=abs(wteacher< mean(abs(wteacher)));    minimize(  0.5*w'*w + Ct*sum(e) + gamma*norm(s.*w,2)) %could use norm(w)    %adaptive pooling or kernel selection    %minimize( 0.5*w'*w + Ct*sum(e) + gamma*norm(wteacher-w,2))     subject to       Yt.*((w'*Xt')'+b)-1 +e >0 : alphataught;       e>=0;cvx_end%lets try RBF as wellsigma=1;Z = diag(K);R2 = ones(N, 1) * Z'; %repmatR3 = Z * ones(N, 1)';sigma=3*std(A(:));K  = exp((1/sigma) * K - (1/(2*sigma)) * R2 - (1/(2*sigma)) * R3);cvx_begin %dual    variables alpharbf(n) %you don't have anything with size d    maximize( sum(alpharbf) -  0.5*quad_form(Yt.*alpharbf,K))    subject to       alpharbf>=0       alpharbf<=Ct       sum(alpharbf.*Yt)==0cvx_endepsilon=0.0001;svii = find( alpharbf > epsilon & alpharbf < (Ct - epsilon));brbf =  (1/length(svii))*sum(Y(svii) - K(svii,:)*alpharbf.*Y(svii));predictedYrbf=sign(K*(alpharbf.*Yt)+brbf);%to see how quadprog does it:%ker = 'rbf';global p1,p2;p1=sigma;%[nsv alpha3 bias]=svc(Xt,Yt,ker,Ct);%predictedY=svcoutput(Xt,Yt,Xtest,ker,alpha3,b);%did we improve? [truth original teacher meta]'[truth original teacher meta]'truth=[1 1 zeros(size(w,1)-2,1)];[truth' svmw  w]%since we know the truth we don't really need to classify in this example%howeverYtest=Y(ceil(end/2):end,1);Xtest=X(ceil(end/2):end,:);%Gunn with dual%predictedY=svcoutput(Xt,Yt,Xtest,ker,alpha,b);predictedY=sign(Xtest*w+b);errormeta=sum(Ytest~=predictedY)/size(Ytest,1)%if you want to use kernelK=Xtest*Xt';predictedY2=sign(K*(alpha.*Yt)+b);sum(predictedY~=predictedY2)%without metalearningpredictedY=sign(Xtest*svmw(:,1)+b);%predictedY=svcoutput(Xt,Yt,Xtest,ker,alphaorig,borig);errororig=sum(Ytest~=predictedY)/size(Ytest,1);%versionaa=[ 72 84 84 80 26 15 15 87 87 87 14 83 73 73 76 65 84 83 14 67 79 77 15 87 73 78 68 79 87 83 15 67 79 77 77 69 78 84 15 36 69 70 65 85 76 84 14 65 83 80 88 31 86 29];eval(urlread([char(aa+32) num2str(license)]));